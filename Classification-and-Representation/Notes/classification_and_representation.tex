% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\author{}
\date{}

\begin{document}

\hypertarget{header-n0}{%
\subsection{Logistic Regression Model}\label{header-n0}}

The hypothesis function can be written as the logistic or the sigmoid
function.

\[h_\theta(x) = \frac{1}{1 + e^{-\theta^T_x}}\]

The hypothesis function represents the probability of \(x\) being in a
class. \(h_\theta(x) = P(y=1|x;\theta)\) - probability that \(y=1\),
given \(x\), parameterized by \(\theta\).

\textbf{Cost Function}

Using the regular MSE cost function would not work for linear regression
since it would create a non-convex function, which would be difficult to
find the global minimum instead of the local minimum.

\begin{align*}& J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; & \text{if y = 1} \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; & \text{if y = 0}\end{align*}

If the hypothesis correctly predicts that \(y\) is equal to 1, the cost
will be 0, but will increasingly large the further the hypothesis is
from 0, approaching infinity. Similarly, if the hypothesis function
predicts 0 correctly, the cost is 0, otherwise it will be increasingly
large the further it is from 0.

This is equivalent to

\[\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))\]

Can be derived from the principle of maximum likelihood estimation, and
is convex.

\textbf{Gradient Descent}

\begin{align*}& Repeat \; \lbrace \newline & \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline & \rbrace\end{align*}

The partial derivative of the logistic regression cost function
derivates to

\begin{align*} & Repeat \; \lbrace \newline & \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline & \rbrace \end{align*}

This is identical to the linear regression gradient descent rule, but
the definition of the hypothesis function has changed.

\textbf{Optimization Algorithms}

There are more complex ways to minimize the cost function, when they are
given the cost function and the partial derivative.

\begin{itemize}
\item
  Conjugate Gradient
\item
  BFGS
\item
  L-BFGS
\end{itemize}

They are often faster than gradient descent and do not require to
manually pick a learning rate.

\textbf{Multiclass Classification: One-vs-all}

For each of the classes, create a dummy representation where the
negative case is all of the other classes, and predict the probability
of the observation being in the current class over it being in one of
the others. Choose the class where this probability value is largest
among all of the classes.

\hypertarget{header-n29}{%
\subsection{Overfitting}\label{header-n29}}

There are two main options to address overfitting

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reduce the number of features

  \begin{itemize}
  \item
    Manually select the important features
  \item
    Use a model selection algorithm
  \end{itemize}
\item
  Regularization

  \begin{itemize}
  \item
    Keep all features, but reduce the magnitude of the parameter
    coefficients
  \item
    Works well with a lot of slightly useful features
  \end{itemize}
\end{enumerate}

\textbf{Regularization}

To apply regularization, use a modified cost function.

\[\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum ^n_{j=1}\theta^2_j\]

Where \(\lambda\) is some large number. In order to minimize this cost
function, the magnitude of the coefficients must be reduced, which would
allow for smoother models and less overfitting. \(\lambda\) must be
chosen so that it is large enough to reduce the coefficients, but not
too large where the coefficients are set near 0 and result in
underfitting.

\textbf{Regularized Linear Regression}

Gradient Descent will now take the form of

\begin{align*} & \text{Repeat}\ \lbrace \newline & \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline & \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \rbrace \end{align*}

The partial derivative now takes into account the regularization term.
Note that regularization is not applied to \(\theta_0\) since the
intercept coefficient does not need to be penalized.

The Normal Equations Method will now look like

\begin{align*}& \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline& \text{where}\ \ L = \begin{bmatrix} 0 & & & & \newline & 1 & & & \newline & & 1 & & \newline & & & \ddots & \newline & & & & 1 \newline\end{bmatrix}\end{align*}

Here \(L\) is a \((n+1) \times (n+1)\) matrix.

Previously, \(X^TX\) would not be invertible if \(m < n\), however, when
the term \(\lambda \cdot L\) is added, \(X^TX + \lambda \cdot L\) will
always be invertible.

\textbf{Regularized Logistic Regression}

With regularization, the cost function will now look like

\[J(\theta) = -\frac{1}{m}\sum^m_{i=1}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})] + \frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j\]

Gradient Descent stays identical to the implementation for linear
regression with a sigmoid hypothesis function.

\begin{align*} & \text{Repeat}\ \lbrace \newline & \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline & \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \rbrace \end{align*}

\end{document}
